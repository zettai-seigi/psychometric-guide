---
layout: default
title: "Chapter 7: Forced-Choice Format and Faking Resistance"
parent: "Part II: OPQ32 Personality"
nav_order: 4
permalink: /chapters/07-forced-choice/
---

# Chapter 7: Forced-Choice Format and Faking Resistance

## Learning Objectives

By the end of this chapter, you will be able to:
- Understand the problem of socially desirable responding and faking in personality assessment
- Recognize the limitations of normative Likert-type scales in high-stakes selection contexts
- Explain how forced-choice formats address the faking problem
- Grasp the rationale behind the OPQ32i ipsative format
- Identify the psychometric problems created by classical ipsative scoring
- Understand the historical evolution from OPQ32n to OPQ32i to OPQ32r

## Introduction: The Challenge of Valid Personality Assessment

Personality assessment in occupational settings faces a fundamental challenge: how do we measure an individual's true characteristics when they have strong incentives to present themselves favorably? This question becomes particularly acute in high-stakes selection contexts, where a job offer, promotion, or career opportunity hangs in the balance. The tension between obtaining valid personality data and the natural human tendency toward self-enhancement has driven decades of methodological innovation in psychometric testing.

The Occupational Personality Questionnaire has evolved through multiple iterations specifically to address this challenge. Understanding this evolution—from normative to forced-choice formats—provides crucial insight into modern personality assessment methodology and the sophisticated solutions that have emerged to maintain measurement validity in competitive environments.

## The Faking Problem in Personality Assessment

### What is Socially Desirable Responding?

**Socially desirable responding** (SDR), also known as "faking good" or "impression management," occurs when individuals consciously or unconsciously present themselves in an overly favorable light on personality assessments. This phenomenon is not simply dishonesty; it reflects the complex interaction between:

1. **Situational Pressures:** The high-stakes nature of selection decisions creates powerful incentives to appear ideal
2. **Self-Perception:** Individuals may genuinely believe they possess desirable traits (self-enhancement bias)
3. **Social Norms:** Cultural expectations about appropriate workplace behavior influence responses
4. **Test Transparency:** When it's obvious which answer is "correct" for a job, respondents adjust accordingly

### The Impact of Faking on Assessment Validity

Research has consistently demonstrated that faking behavior poses a serious threat to the validity of personality assessments in selection contexts:

**Score Inflation:** In high-stakes situations, mean scores on desirable traits (like conscientiousness, emotional stability, and extraversion in leadership roles) increase significantly compared to low-stakes research settings. This inflation can reach 0.5 to 1.0 standard deviations on some scales.

**Reduced Differentiation:** When all candidates "fake up" their scores, the assessment loses its ability to discriminate between truly high and moderate performers on desirable traits. Everyone clusters at the top of the scale, reducing the practical utility of the measure.

**Altered Rank Ordering:** Perhaps most concerning, faking can change the relative ranking of candidates. Those who fake most effectively (not those who actually possess the desired traits) may be selected, inverting the intended relationship between test scores and job performance.

**Diminished Criterion Validity:** As faking increases, the correlation between personality scores and job performance criteria weakens. The assessment measures test-taking sophistication rather than genuine personality characteristics.

### Why Normative Formats Are Vulnerable

The traditional normative format—commonly using Likert scales where respondents rate agreement with statements—is particularly susceptible to faking for several reasons:

**Independent Item Responses:** Each item is answered in isolation. Respondents can endorse all socially desirable statements without constraint. There is no built-in mechanism to force trade-offs.

**Transparent Scoring Direction:** It's typically obvious which direction is favorable. For example, "I am reliable and finish what I start" clearly measures a desirable trait, and "Strongly Agree" is obviously the "correct" answer for a job applicant.

**Acquiescence Bias:** In addition to intentional faking, normative formats are vulnerable to acquiescence bias—the tendency to agree with statements regardless of content. Combined with social desirability, this creates systematic score inflation.

**No Internal Consistency Check:** While some normative tests include validity scales, these are often transparent and can themselves be faked by sophisticated test-takers.

### The OPQ32n: The Normative Version

The original OPQ included a normative version, known as **OPQ32n**, which used either True/False or Likert-type response formats. In this version:

- Candidates rated their agreement with each statement independently
- Scoring was straightforward: ratings on items measuring each trait were summed or averaged
- Classical Test Theory (CTT) methods were used to calculate trait scores
- Scores were then standardized against normative data (converted to Stens)

While the OPQ32n provided scores that were psychometrically sound—allowing for valid between-person comparisons, factor analysis, and regression-based validity studies—it suffered from the vulnerabilities described above. In research settings with no consequences attached to responses, the OPQ32n performed well. However, in actual selection contexts, SHL researchers documented substantial score inflation due to impression management.

## The Forced-Choice Solution

### The Logic of Forced-Choice Formats

To address the limitations of normative testing, SHL introduced the forced-choice format in the **OPQ32i** (where "i" stands for "ipsative"). The fundamental insight was elegantly simple: if you force respondents to choose between statements that are equally socially desirable, it becomes much more difficult to fake.

The logic works as follows:

1. **Matched Social Desirability:** Within each block of statements, all alternatives are carefully matched to be equally attractive or desirable in a work context
2. **Forced Trade-offs:** Respondents must select one statement as "most like me" and another as "least like me," creating a zero-sum game
3. **Controlled Comparison:** By choosing one favorable statement, the respondent must necessarily reject another equally favorable statement
4. **Reduced Transparency:** The "correct" answer is no longer obvious because all options appear equally good

This approach is analogous to asking someone "Would you rather be seen as 'highly organized' or 'socially engaging'?" Both traits are desirable, but you must express a preference. Your pattern of preferences across many such choices reveals your personality profile while resisting simple inflation.

### The OPQ32i Triplet Format

The forced-choice implementation in the OPQ32i used a **triplet format** consisting of 104 blocks, each containing three statements:

![Figure 7.1: Example OPQ32r forced-choice triplet item](/psychometric-guide/images/Figure_07_01.png)

*Figure 7.1: Example OPQ32r forced-choice triplet item showing how candidates must select 'Most like me' and 'Least like me' from three equally desirable statements measuring different traits*

**Structure of Each Block:**
- Three statements, each measuring a different OPQ trait
- Statements matched for social desirability within the block
- Statements carefully balanced to prevent obvious patterns

**Response Requirement:**
- Select the statement that is **"Most Like Me"**
- Select the statement that is **"Least Like Me"**
- The remaining statement is implicitly "middle"

**Example Block** (illustrative):
```
A. I enjoy taking charge of projects (Leadership)
B. I carefully check my work for errors (Detail Conscious)
C. I build strong relationships with colleagues (Affiliative)

Most Like Me: [  ]
Least Like Me: [  ]
```

In this example, all three statements are equally socially desirable in workplace contexts. The respondent must make a genuine choice that reflects their actual preferences and tendencies, rather than simply endorsing everything positive.

### Total Assessment Structure

The complete OPQ32i assessment comprised:
- **104 triplet blocks** (originally some versions used "quads" with four statements, but the triplet format became standard)
- **312 total item responses** (104 × 3 statements)
- **32 trait scales**, each measured across multiple blocks
- **Administration time:** Approximately 30-40 minutes

Each of the 32 personality traits was measured by multiple statements distributed across different blocks. This ensured comprehensive coverage while maintaining the forced-choice structure throughout.

### Demonstrated Faking Resistance

SHL conducted extensive research documenting the effectiveness of the forced-choice format in resisting faking:

**Experimental Studies:** Research comparing individuals in applicant conditions versus honest responding conditions showed substantially smaller differences in forced-choice formats compared to normative formats. The ability to inflate scores was markedly reduced.

**Coaching Studies:** Even when participants were explicitly coached on how to fake personality tests and given information about ideal profiles, the forced-choice format remained relatively resistant to manipulation.

**Practical Selection Contexts:** Analysis of data from real selection settings confirmed that score distributions on the OPQ32i showed less evidence of ceiling effects and maintained better differentiation among candidates compared to normative alternatives.

This empirical evidence solidified the forced-choice format as a critical innovation for high-stakes personality assessment, establishing it as the preferred methodology for organizational selection contexts.

## The Ipsative Scoring Problem

While the forced-choice format successfully addressed the faking problem, it introduced a new set of serious psychometric challenges related to **ipsative scoring**.

### What is Ipsative Data?

The term "ipsative" comes from the Latin "ipse," meaning "self." Ipsative data provides information about an individual's characteristics **relative to their own other characteristics**, rather than relative to other people.

In forced-choice formats, ipsative data arises because of the **constant-sum constraint**: when you select one trait as "most like me," you are simultaneously indicating that the other traits in that block are relatively less characteristic. Your responses are interdependent rather than independent.

### The Constant-Sum Constraint

Under classical scoring methods for the OPQ32i, this interdependence created a mathematical constraint: **the sum of all trait scores for an individual was constant** (or very nearly so, depending on specific scoring rules).

Consider the implications:

- If a person scores high on Trait A, they must necessarily score lower on some other trait(s)
- It's mathematically impossible for someone to score high on all traits or low on all traits
- Scores reflect **relative standing within the person**, not absolute levels

This can be illustrated with a simple example:

```
Individual X responds to forced-choice blocks:
- Chooses "Leadership" over "Analytical" → +1 Leadership, -1 Analytical
- Chooses "Sociable" over "Detail Conscious" → +1 Sociable, -1 Detail Conscious
- Chooses "Leadership" over "Sociable" → +1 Leadership, -1 Sociable

Result: Leadership (+2), Sociable (0), Analytical (-1), Detail Conscious (-1)
Total sum: 0 (constant)
```

No matter how the individual responds, the total sum remains fixed. This mathematical reality has profound consequences.

### Problems with Between-Person Comparisons

The most fundamental problem with classical ipsative scoring is that **it invalidates between-person comparisons**. Consider two individuals:

**Person A:**
- Leadership: Sten 7
- Analytical Thinking: Sten 5
- Sociability: Sten 4

**Person B:**
- Leadership: Sten 7
- Analytical Thinking: Sten 6
- Sociability: Sten 3

Under ipsative scoring, both show Leadership as Sten 7. But what does this mean?

- For Person A: Leadership is high **relative to their own other traits**
- For Person B: Leadership is high **relative to their own other traits**
- We **cannot conclude** that Person A and Person B have the same absolute level of leadership potential

Person A might be genuinely high on leadership compared to the general population, while Person B might be moderate on leadership but very low on other traits, making leadership appear high by comparison. The Sten 7 scores are **not comparable** between persons.

This limitation is devastating for selection purposes, where the goal is precisely to rank-order candidates and compare them to one another.

### Statistical Distortions Under Classical Ipsative Scoring

The interdependence of ipsative scores creates multiple statistical problems:

**1. Distorted Reliability Estimates**

Traditional reliability calculations (like Cronbach's alpha) assume that item responses are independent. Ipsative data violates this assumption, typically leading to artificially deflated reliability estimates that underestimate the true consistency of measurement.

**2. Invalid Factor Analysis**

Factor analysis, which is used to examine the underlying structure of personality traits, produces misleading results with ipsative data. The constant-sum constraint creates artificial negative correlations between traits, distorting the factor structure and making it difficult to confirm theoretical models like the Big Five.

**3. Problematic Correlations**

Correlation coefficients between ipsative trait scores and external criteria (like job performance) are difficult to interpret. Negative correlations can appear even when no true negative relationship exists, simply as an artifact of the constant-sum constraint.

**4. Incompatibility with Regression Analysis**

Multiple regression analysis—a cornerstone of validity research and predictive modeling—becomes problematic with ipsative scores. The interdependence among predictors violates the assumption of independent variables, leading to unstable coefficients and misleading conclusions.

### The Validity Paradox

This created a paradox for occupational assessment:

- **Normative formats** provided psychometrically sound, comparable scores suitable for selection decisions, but were vulnerable to faking, potentially undermining validity
- **Ipsative formats** effectively resisted faking and maintained measurement integrity, but produced scores that couldn't validly be used for the very selection decisions they were designed to support

For over a decade, this trade-off represented an unsatisfactory compromise. Organizations using the OPQ32i had to accept that while the test was measuring genuine personality differences (not just faking ability), the scores themselves had limited utility for direct candidate comparison.

### Practical Workarounds and Their Limitations

Practitioners attempted various workarounds to use ipsative data in selection:

**Profile Interpretation:** Rather than comparing specific trait scores, focus on the overall pattern or profile of scores. While this provided some utility for structured interviews and development conversations, it did not solve the comparison problem.

**Rank-Ordering Within Traits:** Some researchers proposed using only the rank-order of individuals within a specific trait, ignoring score differences. However, this approach sacrificed substantial information and remained theoretically questionable.

**Normative Conversion Tables:** Attempts were made to convert ipsative scores to normative equivalents using statistical transformations. These were only partially successful and could not fully recover the normative information lost in the forced-choice format.

None of these approaches satisfactorily resolved the fundamental psychometric problems. A more sophisticated solution was needed—one that could genuinely recover normative scoring properties from forced-choice data.

## Historical Evolution: From OPQ to OPQ32i to OPQ32r

Understanding the progression of the OPQ provides valuable context for the methodological breakthrough achieved in the OPQ32r (covered in Chapter 8).

### Phase 1: Early OPQ and Normative Formats (1980s-1990s)

**Initial Development:**
- The original OPQ was developed in the 1980s as a comprehensive work-focused personality assessment
- Multiple versions existed for different job levels (managerial, professional, etc.)
- Used normative Likert scales and Classical Test Theory (CTT) scoring
- Scores were fully comparable between individuals

**Recognized Limitations:**
- Research and practical experience revealed substantial faking in selection contexts
- Growing awareness in the broader psychometric community about the vulnerability of normative personality measures
- Need for a more fake-resistant format became increasingly apparent

### Phase 2: Introduction of OPQ32i (Late 1990s-2000s)

**The Major Shift:**
- Around the late 1990s, SHL introduced the OPQ32i with forced-choice format
- This was framed as a major methodological advance for "fraud-resistant" selection
- Positioned as the solution to impression management concerns
- Rapidly adopted as the standard version for high-stakes assessment

**Initial Success and Growing Concerns:**
- The OPQ32i successfully reduced faking, as documented in validation studies
- However, psychometricians raised concerns about the ipsative scoring limitations
- Published critiques highlighted the statistical problems with between-person comparisons
- Practitioners struggled with how to use ipsative scores appropriately in selection

**The Methodological Challenge:**
- By the mid-2000s, it was clear that while the forced-choice format was valuable, classical ipsative scoring was inadequate
- The field needed a way to maintain faking resistance while recovering normative score properties
- This set the stage for a major psychometric innovation

### Phase 3: Breakthrough to OPQ32r (2009-2010)

**The Watershed Moment:**
- Around 2009-2010, SHL released the OPQ32r, representing a methodological breakthrough
- The "r" designation indicated a new revision using advanced psychometric modeling
- Applied Thurstonian Item Response Theory to forced-choice data
- Fundamentally transformed what was possible with forced-choice assessment

**Key Innovations:**
- **Refined Triplet Format:** Streamlined to 104 triplet blocks (reduced from some earlier longer versions)
- **IRT Scoring Algorithm:** Implemented the Multi-Unidimensional Pairwise Preference (MUPP) model
- **Normative Score Recovery:** Successfully extracted normative-equivalent scores from forced-choice data
- **Reduced Length:** Achieved 40-50% reduction in assessment length while maintaining reliability

This evolution solved the validity paradox, combining the best of both approaches: the faking resistance of forced-choice with the psychometric soundness and comparability of normative scoring.

## The Need for Advanced Scoring

The limitations of classical ipsative scoring created the imperative for methodological innovation. By the mid-2000s, several converging factors made a breakthrough both necessary and possible:

### The Research Foundation

**Academic Developments:**
- Psychometric theory had advanced significantly, with sophisticated IRT models capable of handling complex data structures
- Thurstone's (1927) original work on paired comparisons and the "law of comparative judgment" provided theoretical foundations
- New computational algorithms made it feasible to estimate multidimensional models with large numbers of parameters

**Published Critiques:**
- Academic literature increasingly highlighted the problems with ipsative data
- Researchers called for new approaches that could overcome the constant-sum constraint
- The field recognized that forced-choice formats had value, but the scoring methodology needed fundamental revision

### Practical Pressures

**Organizational Needs:**
- Organizations demanded assessments that were both valid (faking-resistant) and useful (yielding comparable scores)
- The inability to meaningfully rank-order candidates on ipsative scores was increasingly seen as untenable
- Competitive pressure from alternative assessments that didn't face these limitations

**Regulatory and Professional Standards:**
- Professional guidelines (e.g., Standards for Educational and Psychological Testing) emphasized the need for validity evidence
- The questionable validity of between-person comparisons with ipsative scores raised compliance concerns
- The field needed a defensible methodology that met the highest psychometric standards

### The Convergence Toward Thurstonian IRT

The solution emerged from the intersection of:
1. **Theoretical insights** from Thurstonian scaling and comparative judgment
2. **Methodological advances** in multidimensional IRT modeling
3. **Computational capabilities** enabling estimation of complex models with large samples
4. **Practical validation** demonstrating that normative scores could indeed be recovered

This convergence led to the development and implementation of the Thurstonian IRT model for the OPQ32r, which successfully addressed the psychometric problems while preserving faking resistance.

## Implications for Modern Practice

Understanding the forced-choice format and its evolution has important implications for test users:

### Appropriate Use of Different OPQ Versions

**OPQ32r (Current Standard):**
- Recommended for all high-stakes selection and promotion decisions
- Combines faking resistance with valid between-person comparisons
- Scores can be used in the same way as normative data (regression, comparisons, cut-scores)

**OPQ32n (Legacy):**
- May still be appropriate for low-stakes developmental contexts where faking is not a concern
- Provides slightly more straightforward interpretation for coaching conversations
- Should not be used in competitive selection where impression management is likely

### Interpretation Considerations

**What the Forced-Choice Format Measures:**
- Genuine personality preferences and behavioral tendencies
- Relative priorities among traits when trade-offs must be made
- More authentic reflection of how individuals actually navigate competing demands

**What It Controls For:**
- Conscious impression management and faking
- Acquiescence bias and extreme response styles
- Transparent "correct" answers that undermine validity

### Communicating with Candidates

When using the OPQ32r, it's helpful to explain to candidates:
- The format requires trade-offs between equally attractive options
- This design helps ensure the assessment measures genuine personality characteristics
- There are no "right" answers; the goal is to understand their authentic preferences
- The sophisticated scoring methodology enables fair comparison while maintaining integrity

## Key Takeaways

1. **Socially desirable responding (faking) is a significant threat** to the validity of personality assessment in high-stakes selection contexts, potentially distorting scores, reducing differentiation, and altering rank-orderings.

2. **Normative Likert-type formats (OPQ32n) are vulnerable to faking** because items are answered independently, scoring direction is transparent, and there are no constraints forcing trade-offs between desirable traits.

3. **The forced-choice format (OPQ32i) effectively reduces faking** by requiring respondents to choose between statements matched for social desirability, creating a zero-sum situation where endorsing one favorable trait means rejecting another.

4. **The triplet format uses 104 blocks** of three statements, where respondents select "Most Like Me" and "Least Like Me," providing comprehensive personality measurement while maintaining faking resistance throughout the assessment.

5. **Classical ipsative scoring creates serious psychometric problems:** scores are interdependent (constant-sum constraint), between-person comparisons become invalid, and statistical analyses like factor analysis and regression are distorted.

6. **The validity paradox:** normative formats provided comparable scores but were vulnerable to faking; ipsative formats resisted faking but produced incomparable scores—neither solution was satisfactory on its own.

7. **Historical evolution from OPQ32n to OPQ32i to OPQ32r** reflects the ongoing effort to balance measurement validity (faking resistance) with statistical validity (comparable scores), culminating in the methodological breakthrough of Thurstonian IRT.

8. **The limitations of classical ipsative scoring** created the imperative for advanced psychometric modeling, setting the stage for the Thurstonian IRT solution implemented in the OPQ32r (Chapter 8).

---

## Chapter Navigation

[← Previous: Chapter 6 - Big Five Congruence and Validation](/psychometric-guide/chapters/06-big-five/)

[Next: Chapter 8 - Thurstonian IRT Breakthrough →](/psychometric-guide/chapters/08-thurstonian-irt/)

[↑ Back to Home](/psychometric-guide/)
