---
layout: default
title: "Chapter 10: Verify Suite Overview"
parent: "Part III: Verify Ability Tests"
nav_order: 1
permalink: /chapters/10-verify-overview/
---

# Chapter 10: Verify Suite Overview
{: .no_toc }

## Table of Contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Learning Objectives

By the end of this chapter, you will be able to:

1. Understand the historical context and R&D investment behind the Verify suite (2006-2007)
2. Recognize General Mental Ability (GMA) and its predictive power in workplace settings
3. Identify the three core reasoning domains measured by Verify
4. Explain the Verify product suite structure and component tests
5. Describe how Verify compares to competitive offerings from Talent Q and Saville
6. Grasp the measurement taxonomy that underlies workplace cognitive assessment
7. Understand the role of Verify within SHL's integrated assessment ecosystem

---

## Introduction: The Digital Assessment Revolution

The mid-2000s marked a pivotal moment in psychometric assessment. As organizations increasingly moved their hiring processes online, a critical challenge emerged: how could cognitive ability tests maintain their security, validity, and efficiency in unsupervised digital environments? Traditional paper-based tests, with their fixed forms and simple number-correct scoring, were ill-equipped for this new reality.

SHL's response to this challenge was the **Verify suite**, introduced around 2006-2007 as a modern approach to cognitive ability testing for employment contexts. The Verify range represents what the sources describe as a **"huge R&D effort in the mid-2000s"**—a substantial investment in research and development focused on creating secure, precise, and efficient cognitive assessments for the digital age.

The name "Verify" itself signals the suite's innovative approach to security: a two-stage verification model designed to confirm that unsupervised online scores represent genuine candidate ability rather than cheating or coaching. This security-first design philosophy, combined with cutting-edge psychometric methodology, positioned Verify at the forefront of cognitive assessment technology.

The Verify suite measures cognitive abilities and General Mental Ability (the "g factor") that predicts job performance, relying on modern probabilistic modeling rather than classical test theory. This chapter explores the theoretical foundations, product structure, and competitive positioning of this influential assessment system.

---

## The g Factor and Workplace Performance

### Theoretical Foundations

At the heart of the Verify suite lies a fundamental psychological construct: **General Mental Ability (GMA)**, often referred to as the **g factor**. First identified by Charles Spearman in the early 20th century, the g factor represents the common variance shared across diverse cognitive tasks. When people perform well on one type of reasoning task, they tend to perform well on others—this positive correlation across cognitive domains suggests an underlying general intelligence factor.

Modern cognitive psychology, particularly Carroll's three-stratum theory of intelligence, conceptualizes cognitive abilities as hierarchical:

- **Stratum III (Bottom):** Narrow, specific abilities (e.g., reading comprehension, mental arithmetic)
- **Stratum II (Middle):** Broad abilities (e.g., fluid intelligence, crystallized intelligence)
- **Stratum I (Top):** General mental ability (g)

The g factor sits at the apex of this hierarchy, representing the most general level of cognitive functioning. It reflects the efficiency of the brain's information-processing systems and the capacity to learn, reason, solve problems, and adapt to novel situations.

### Predictive Power in the Workplace

Why does GMA matter for employee selection? Meta-analyses spanning decades of research consistently demonstrate that cognitive ability tests have **strong predictive validity for job performance** across virtually all occupations. The predictive power of GMA is not merely statistically significant—it is among the strongest single predictors available to organizational decision-makers.

Several factors explain this robust relationship:

1. **Learning Speed:** Higher GMA enables faster acquisition of job knowledge, reducing training time and costs
2. **Problem-Solving:** Complex jobs require analyzing information, identifying patterns, and generating solutions
3. **Adaptability:** The modern workplace demands flexibility and the ability to master new technologies and processes
4. **Decision Quality:** Many roles require evaluating options under uncertainty and time pressure
5. **Universal Relevance:** Unlike specific skills that apply to narrow job families, reasoning ability matters across occupations

The sources note that "cognitive tests have strong predictive validity for job performance," and "most top vendors achieve similar validity if their tests are well-designed." Meta-analytic research typically finds operational validities ranging from 0.30 to 0.60, with even higher values when corrected for range restriction and criterion unreliability.

Importantly, GMA's predictive power is not limited to entry-level positions. While the correlation between ability and performance is strongest for complex jobs, even relatively simple roles benefit from having employees who can learn quickly, solve problems efficiently, and adapt to changing circumstances.

### The Measurement Challenge

Despite its theoretical clarity and empirical support, measuring GMA in employment contexts presents several challenges:

- **Test Length vs. Precision:** Traditional tests required many items to achieve acceptable reliability, creating time burdens
- **Security:** Fixed-form tests could be compromised through item sharing or coaching services
- **Fairness:** Tests needed to measure cognitive ability while minimizing cultural bias and adverse impact
- **Candidate Experience:** Lengthy, repetitive tests created negative impressions and dropout
- **Scoring Comparability:** Different test forms needed to produce equivalent scores despite varying difficulty

The Verify suite was engineered specifically to address these challenges through the application of advanced psychometric methodology, particularly Item Response Theory (IRT) and Computer Adaptive Testing (CAT). These technical solutions are explored in depth in Chapters 12 and 13.

---

## The Measurement Taxonomy: Three Reasoning Domains

The Verify suite decomposes general intelligence into three specific reasoning mechanisms, each representing a distinct cognitive capacity relevant to workplace performance. These tests are designed around a **measurement taxonomy** that defines how these abilities manifest in workplace tasks.

### Numerical Reasoning

**Numerical Reasoning** assesses the ability to interpret, analyze, and draw logical conclusions from quantitative information presented in tables, charts, and graphs. This domain measures more than simple arithmetic calculation—it evaluates the capacity to extract meaning from numerical data and apply logical reasoning to business scenarios.

**Typical Item Content:**
- Interpreting business charts (bar graphs, line graphs, pie charts)
- Analyzing tables of financial or operational data
- Calculating percentages, ratios, and proportions
- Making comparisons and identifying trends
- Drawing inferences from incomplete data sets

**Workplace Applications:**
Numerical reasoning ability predicts performance in roles requiring data interpretation, financial analysis, budgeting, forecasting, quality control, and evidence-based decision-making. In modern data-driven organizations, the ability to "read" quantitative information is increasingly essential across job levels.

**Cognitive Processes Engaged:**
- Working memory (holding multiple values simultaneously)
- Quantitative reasoning (understanding relationships between numbers)
- Logical inference (determining what must, might, or cannot be true)
- Pattern recognition (identifying trends in data)
- Attention to detail (avoiding calculation errors)

### Verbal Reasoning

**Verbal Reasoning** measures the ability to evaluate logical arguments, assess evidence, and draw sound conclusions from written information. Verify's verbal reasoning tests typically use the classic **True/False/Cannot Say** format, where candidates read a passage and evaluate statements about it.

**Response Options Explained:**
- **True:** The statement logically follows from the passage
- **False:** The statement contradicts the passage or is inconsistent with it
- **Cannot Say:** The passage provides insufficient information to determine truth or falsity

**Typical Item Content:**
- Business reports and memos
- Policy statements and procedures
- Market analysis and research findings
- Logical arguments and reasoning chains
- Technical documentation

**Workplace Applications:**
Verbal reasoning predicts performance in roles requiring document analysis, policy interpretation, legal reasoning, strategic planning, report writing, and critical evaluation of arguments. It is particularly predictive for managerial and professional roles.

**Cognitive Processes Engaged:**
- Reading comprehension (understanding text meaning)
- Deductive reasoning (applying logical rules)
- Critical thinking (evaluating argument quality)
- Information integration (combining multiple facts)
- Distinguishing inference from assumption

**The Deductive Logic Challenge:**
Verbal reasoning items require strict logical thinking rather than general knowledge or opinion. A statement may be "Cannot Say" even if it seems probable based on common sense—the candidate must base judgments solely on the information explicitly provided or logically entailed by the passage.

### Inductive Reasoning (Diagrammatic Reasoning)

**Inductive Reasoning**, sometimes called Diagrammatic or Abstract Reasoning, assesses the ability to identify patterns, rules, and relationships in novel, non-verbal material. This domain measures fluid intelligence—the capacity to solve problems in unfamiliar situations without relying on learned knowledge.

**Typical Item Content:**
- Sequences of abstract shapes or symbols
- Matrix completion problems (identifying missing elements)
- Rule discovery (determining transformation patterns)
- Spatial relationships and rotations
- Logical series and progressions

**Workplace Applications:**
Inductive reasoning predicts performance in roles requiring innovation, systems thinking, troubleshooting, strategic planning, and adaptation to novel situations. It is particularly relevant for technical roles, problem-solving positions, and leadership contexts requiring vision and conceptualization.

**Cognitive Processes Engaged:**
- Pattern recognition (identifying regularities)
- Rule induction (inferring underlying principles)
- Hypothesis testing (evaluating candidate rules)
- Abstract thinking (reasoning without verbal content)
- Mental flexibility (considering multiple interpretations)

**Why Measure Inductive Reasoning?**
Unlike verbal and numerical tests, which can be influenced by educational background and domain knowledge, inductive reasoning provides a relatively "culture-fair" measure of raw problem-solving capacity. It assesses how well candidates can think on their feet when confronted with genuinely novel problems—a critical skill in rapidly changing work environments.

### The Integration of Domains

While each domain can be assessed independently, the three reasoning types share substantial common variance attributable to the g factor. A candidate with high GMA will typically perform well across all three domains, though individuals may show relative strengths. The three-domain structure allows organizations to:

1. **Tailor Assessment:** Select domain-specific tests when job analysis indicates particular reasoning types are critical
2. **Comprehensive Measurement:** Use combined tests (Verify G+) for a holistic GMA assessment
3. **Profile Interpretation:** Identify relative cognitive strengths and development areas
4. **Fairness Enhancement:** Offer multiple pathways to demonstrate ability, reducing the impact of specific educational backgrounds

---

## Verify Product Suite Structure

The Verify range encompasses multiple product configurations designed to meet diverse organizational needs, from rapid screening to comprehensive assessment.

### Verify G+ Combined Test

The flagship product is **Verify G+**, a combined test that integrates measures across all three reasoning domains: Numerical, Verbal, and Inductive Reasoning. This test provides a single, flexible measure of general mental ability suitable for varied organizational needs.

**Structure:**
- **30 items total:** 10 Numerical + 10 Verbal + 10 Inductive
- **Shuffled presentation:** Items from all three domains are interleaved rather than blocked by type
- **Adaptive administration:** The test uses Computer Adaptive Testing algorithms (detailed in Chapter 13)
- **Typical completion time:** 35-40 minutes, though exact time varies based on adaptive routing

**Design Philosophy:**
Verify G+ was designed as a **general measure** suitable for candidates **across all job levels**, from entry-level positions to executive roles. This broad applicability distinguishes it from some competitors (discussed below) that target specific candidate populations.

**Scoring Output:**
The test yields both a **composite GMA score** and **subscores for each domain**. This dual-level reporting allows organizations to evaluate overall cognitive ability while also identifying domain-specific strengths and weaknesses.

**When to Use Verify G+:**
- Selection decisions where GMA is a primary criterion
- Cross-functional roles requiring diverse cognitive skills
- Early-stage screening before more specialized assessment
- Succession planning and talent identification programs
- Situations requiring a comprehensive ability profile

### Domain-Specific Tests

In addition to the combined G+ test, Verify offers standalone assessments for each reasoning domain. These tests typically include more items per domain, providing enhanced precision for domain-specific measurement.

**Verify Numerical Reasoning:**
- 20-30 items focusing exclusively on quantitative reasoning
- Deeper assessment of numerical ability
- Suitable for roles with heavy quantitative demands (e.g., finance, analytics, engineering)

**Verify Verbal Reasoning:**
- 20-30 items focusing exclusively on verbal reasoning
- Enhanced precision for language-dependent roles
- Suitable for positions requiring document analysis, writing, or legal reasoning

**Verify Inductive Reasoning:**
- 20-30 items focusing exclusively on pattern recognition and abstract reasoning
- Suitable for technical, strategic, and innovation-focused roles

**When to Use Domain-Specific Tests:**
- Job analysis indicates one domain is particularly critical
- Additional precision needed for high-stakes decisions
- Validation research shows differential prediction by domain
- Supplementing other assessments to round out the profile
- Roles with narrow, specialized cognitive demands

### Verify Interactive

More recently, SHL developed **Verify Interactive**, an evolved format optimized for mobile devices and incorporating activity-based assessments. Verify Interactive G+ utilizes **24 drag-and-drop manipulation tasks**, requiring candidates to **create answers rather than select from options**.

**Key Features:**
- **Mobile-first design:** Optimized for tablets and smartphones
- **Drag-and-drop interaction:** Enhanced engagement compared to multiple-choice
- **Game-like elements:** Increased face validity and candidate experience
- **Process data capture:** Records speed, error correction, and solution strategies
- **Seamless adaptive scoring:** One continuous session rather than separate verification stage

The Interactive format represents the evolution of Verify from traditional linear algorithms to fully adaptive, engaging digital assessments that enhance both precision and candidate experience.

### Other Verify Tests

Beyond the core reasoning domains, the Verify suite includes specialized assessments for specific abilities:

- **Verify Checking:** Error detection and data verification
- **Verify Mechanical Comprehension:** Understanding of physical principles
- **Verify Calculation:** Pure computational speed and accuracy

Some of these specialized tests may use classical scoring rather than adaptive algorithms, as the constructs being measured differ from broad reasoning abilities.

---

## Competitive Positioning: Verify vs. Talent Q and Saville

The cognitive ability assessment market is characterized by methodological convergence on modern psychometrics (IRT and adaptive testing) but divergence in test design philosophy and target populations. Understanding how Verify compares to its primary competitors provides insight into SHL's strategic positioning.

### Talent Q Elements (Now Korn Ferry)

**Talent Q's Elements series** (logical, numerical, and verbal) was recognized as **one of the first to introduce adaptive ability tests in the early 2010s**, adopting an approach similar to Verify's. Both systems share a methodological foundation in IRT-calibrated item banks and dynamically adjusted difficulty.

**Methodological Similarities:**
- IRT-based scoring (theta estimates)
- Computer adaptive testing algorithms
- Emphasis on efficiency through adaptive item selection
- Strong predictive validity for job performance

**Key Distinction—Design Philosophy:**
Talent Q's approach allowed for **longer tests with more difficult items tailored to higher-level candidates** to effectively **differentiate top performers**. This design choice reflects a strategic focus on senior leadership and executive assessment, where fine discrimination at the high end of the ability distribution is critical.

In contrast, Verify G+ aimed for a **more general measure** that could be used across all job levels, focusing on striking a balance in difficulty range across the entire assessment. This broader applicability makes Verify suitable for high-volume recruitment across diverse roles.

**Market Positioning:**
- **Talent Q (Korn Ferry):** Premium positioning, executive search, leadership pipeline
- **SHL Verify:** Broader market, volume recruitment, all organizational levels

### Saville Swift Assessments

**Saville's Swift tests** distinguish themselves through emphasis on **speed as well as power** (accuracy). Swift tests often have **short time limits** and measure the **number of correct answers completed quickly**, making them particularly **appealing for volume recruitment** scenarios.

**Key Features:**
- IRT-based like Verify and Talent Q
- Emphasis on rapid completion
- Suitable for high-volume, early-stage screening
- Speed as a construct of interest (not just a constraint)

**Contrast with Verify:**
While Verify tests incorporate a speed element through timing, they typically **focus on accuracy under timed conditions** rather than pure speed. The Verify approach prioritizes measurement precision across the ability range over maximum efficiency in administration time.

**Market Positioning:**
- **Saville Swift:** Volume recruitment, operational roles, speed-valued contexts
- **SHL Verify:** Balanced approach, diverse organizational applications

### Classical Fixed-Form Competitors

Not all ability tests have adopted modern psychometrics. Tests such as **Pearson's Watson-Glaser** critical thinking test and **Hogan Business Reasoning Inventory (HBRI)** rely on **classical scoring** and **fixed forms**. These classical tests are typically **longer and not adaptive**, contrasting sharply with the efficient, IRT-powered, shorter assessments offered by SHL, Talent Q, and Saville.

**Efficiency Comparison:**
The sources note that adaptive testing (CAT) allows tests to **achieve the same reliability as a fixed-form test with 50% fewer items**. This efficiency advantage translates to:
- Reduced candidate fatigue
- Better completion rates
- Improved candidate experience
- Lower dropout in multi-stage selection

### Validity Convergence

Despite these methodological and design differences, an important finding emerges from the research: **all major ability tests achieve similar validity** if well-designed. Meta-analyses consistently show that cognitive tests from SHL, Talent Q, Saville, and other reputable publishers demonstrate **strong predictive validity for job performance**, typically in the range of operational validities from 0.30 to 0.50.

This validity convergence suggests that while methodology affects efficiency, security, and candidate experience, the fundamental construct being measured—General Mental Ability—is robust across well-constructed assessments. The competitive differentiation comes not from predictive power but from:

- Test administration efficiency
- Security and verification protocols
- Candidate experience and face validity
- Integration with broader talent systems
- Pricing and service models
- Technological innovation and mobile optimization

---

## Verify's Role in the SHL Ecosystem

The Verify suite does not stand alone—it functions as a critical component of SHL's integrated assessment ecosystem, centered on the **Universal Competency Framework (UCF)**.

### Integration with the UCF

The UCF is SHL's **overarching model of work competencies** and serves as the **criterion-centric architecture** connecting all SHL assessments (OPQ32, Verify, MQ) to occupational performance. The framework structures workplace competencies into three hierarchical tiers:

- **Tier 1:** The Great Eight competency factors (e.g., Leading and Deciding, Analyzing and Interpreting)
- **Tier 2:** 20 specific competency dimensions
- **Tier 3:** 96-112 granular behavioral components

Verify ability tests serve as **predictors of competence** because competence is ultimately underpinned by cognitive abilities. The sources explicitly state that Verify tests predict four competencies most strongly:

1. **Analyzing and Interpreting** (strongest correlation, ρ = 0.40)
2. **Creating and Conceptualizing** (ρ = 0.24)
3. **Interacting and Presenting** (when combined with personality)
4. **Formulating Strategies and Concepts**

### Multi-Source Data Synthesis

The power of the UCF lies in its ability to integrate **multi-source data**, combining personality (OPQ), ability (Verify), and motivation (MQ) scores to calculate a **Competency Potential Score**. This integration substantially enhances predictive validity:

- **Personality-only predictors:** operational validities ranging from ρ = 0.16 to ρ = 0.28
- **Combined personality + ability predictors:** validities increase significantly
  - Analyzing and Interpreting: ρ = 0.44
  - Interacting and Presenting: ρ = 0.40
  - Creating and Conceptualizing: ρ = 0.36

The complementary nature of personality and ability data reflects a fundamental truth: workplace performance depends on both **capacity** (can they do it?) and **inclination** (will they do it?). A candidate may have the cognitive horsepower to analyze complex data but lack the conscientiousness to follow through. Conversely, a highly motivated individual may struggle if cognitive demands exceed their ability level.

### Universal Competency Reports (UCR)

The practical output of this integration is the **Universal Competency Report (UCR)**, which translates psychometric scores into the language of workplace competencies. These reports:

- Plot competency potential on a 1-10 scale
- Provide narrative explanations linking traits and abilities to competency predictions
- Offer behaviorally anchored descriptions
- Support selection, development, and succession planning decisions

The UCR represents the translation layer between abstract psychometric constructs (sten scores, theta estimates) and actionable talent decisions. This reporting infrastructure is a key competitive advantage, as it reduces the interpretation burden on hiring managers and provides consistent, research-based predictions.

---

## The Verify Development Process

Understanding how Verify tests are constructed provides insight into their quality and validity.

### Item Authoring and Content Validity

The development process begins with defining the **measurement taxonomy**—the conceptual framework specifying what abilities will be measured and how they manifest in workplace contexts. Item writers (typically psychologists with psychometric training) author large item banks designed to:

- Reflect workplace-relevant content (business scenarios, realistic data)
- Cover a range of difficulty levels (from easy to very challenging)
- Minimize cultural bias and construct-irrelevant variance
- Engage candidates through realistic, meaningful content

For numerical reasoning, this means using business charts, financial tables, and operational data rather than abstract math problems. For verbal reasoning, it means realistic business communications rather than literary passages.

### Pilot Testing and Calibration

Once items are authored, they undergo extensive pilot testing with large samples (the sources mention **approximately 9,000 candidates** in calibration studies). During pilot testing:

1. Items are administered to diverse candidate pools
2. Response data is analyzed for psychometric properties
3. IRT parameters (difficulty and discrimination) are estimated
4. Items failing quality criteria are rejected or revised
5. Remaining items form the calibrated item bank

Chapter 12 explores the IRT calibration process in detail, including the selection of the **2-parameter logistic model (2PL)** after testing alternatives.

### Continuous Validation

The construction process is ongoing. Each Verify test undergoes validation to ensure it predicts relevant outcomes (e.g., job performance) and that content is fair and free of cultural bias. The sources report:

- **Criterion validity:** Weighted operational validities of 0.50 for Verbal and 0.39 for Numerical reasoning
- **Reliability:** Median internal consistency typically ranging from 0.80 to 0.84
- **Continuous calibration:** IRT parameters are updated as new data is collected to ensure accuracy

This commitment to continuous validation ensures that Verify remains psychometrically sound even as candidate populations and organizational contexts evolve.

---

## Key Takeaways

1. **Substantial R&D Investment:** Verify represents a "huge R&D effort in the mid-2000s," introduced around 2006-2007 to meet the challenges of digital, unsupervised assessment

2. **GMA Primacy:** General Mental Ability (g factor) is among the strongest predictors of job performance across virtually all occupations, providing the theoretical foundation for cognitive assessment

3. **Three-Domain Structure:** Verify assesses Numerical, Verbal, and Inductive reasoning through a measurement taxonomy defining how abilities manifest in workplace tasks

4. **Product Flexibility:** The suite includes combined tests (Verify G+), domain-specific assessments, and interactive formats optimized for different use cases and candidate experiences

5. **Competitive Positioning:** Verify aims for broad applicability across job levels, contrasting with Talent Q's focus on senior differentiation and Saville Swift's emphasis on speed

6. **Methodological Convergence:** Leading vendors (SHL, Talent Q, Saville) all use IRT and adaptive testing, with competition centered on design philosophy, security, and candidate experience

7. **Ecosystem Integration:** Verify functions within SHL's UCF-based ecosystem, combining with personality (OPQ) and motivation (MQ) data to predict workplace competencies

8. **Validity Convergence:** Despite methodological differences, well-designed ability tests from major vendors achieve similar predictive validity for job performance

9. **Continuous Evolution:** From traditional formats to Verify Interactive, the suite has evolved to embrace mobile-first design, enhanced engagement, and seamless adaptive scoring

10. **Evidence-Based Design:** Large-scale calibration studies, criterion validation research, and continuous monitoring ensure psychometric quality and fairness

---

## Chapter Navigation

[← Previous: Chapter 9 - Cross-Cultural Adaptation](/psychometric-guide/chapters/09-cross-cultural/)

[Next: Chapter 11 - Cognitive Domains and the g Factor →](/psychometric-guide/chapters/11-cognitive-domains/)

[↑ Back to Home](/psychometric-guide/)
